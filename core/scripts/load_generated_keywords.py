#!/usr/bin/env python3
"""
Load generated keywords from CSV/JSONL into the database.

This script:
1. Reads keywords generated by generate_keywords.py
2. Classifies them as primary (intent) or secondary (places/topics)
3. Loads them into the database using KeywordRepository

Usage:
    python scripts/load_generated_keywords.py --keywords keywords.csv
    python scripts/load_generated_keywords.py --keywords keywords.jsonl --format jsonl
"""
import sys
import argparse
import csv
import json
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.db.session import SessionLocal, init_db
from src.db.models import Keyword
from src.keywords.repository import KeywordRepository
from src.utils.logging import get_logger

logger = get_logger(__name__)

# High-intent phrases that indicate someone wants to buy a home
HIGH_INTENT_PHRASES = [
    'buy a house',
    'buying a house',
    'buy home',
    'buy property',
    'first time home buyer',
    'first-time homebuyer',
    'preapproval',
    'pre-approval',
    'down payment',
    'making an offer',
    'bidding war',
    'mortgage rates',
    'closing costs',
    'home inspection',
    'contingencies',
    'should I rent or buy',
    'rent vs buy',
]

# Company/advertisement indicators to exclude
AD_INDICATORS = [
    'real estate agent',
    'realtor',
    'real estate company',
    'mortgage broker',
    'lender',
    'for sale by owner',
    'fsbo',
    'listing',
    'open house',
    'contact us',
    'call now',
    'visit our website',
    'www.',
    'http://',
    'https://',
    '.com',
    '.net',
    '.org',
    'email',
    '@',
]


def is_high_intent(keyword: str) -> bool:
    """Check if keyword indicates high intent (person wanting to buy home)"""
    keyword_lower = keyword.lower()
    return any(phrase in keyword_lower for phrase in HIGH_INTENT_PHRASES)


def is_advertisement(keyword: str) -> bool:
    """Check if keyword indicates company advertisement"""
    keyword_lower = keyword.lower()
    return any(indicator in keyword_lower for indicator in AD_INDICATORS)


def load_keywords_from_csv(file_path: str) -> list:
    """Load keywords from CSV file"""
    keywords = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            keywords.append({
                'keyword': row.get('keyword', '').strip(),
                'score': int(row.get('score', 0)),
                'template': row.get('template', ''),
                'place1': row.get('place1', ''),
                'place2': row.get('place2', ''),
            })
    return keywords


def load_keywords_from_jsonl(file_path: str) -> list:
    """Load keywords from JSONL file"""
    keywords = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                keywords.append({
                    'keyword': data.get('keyword', '').strip(),
                    'score': data.get('score', 0),
                    'template': data.get('template', ''),
                    'place1': data.get('place1', ''),
                    'place2': data.get('place2', ''),
                })
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse JSON line: {e}")
                continue
    return keywords


def classify_keyword(keyword: str, template: str, place: str) -> tuple:
    """
    Classify keyword as primary (intent) or secondary (place/topic).
    
    Strategy:
    - Primary keywords: Full phrases with intent + place (e.g., "buy a house in Los Angeles")
    - Secondary keywords: Just the place names (e.g., "Los Angeles")
    
    Returns:
        (keyword_type, is_valid)
        - keyword_type: 'primary' or 'secondary'
        - is_valid: True if keyword should be included (not an ad)
    """
    keyword_lower = keyword.lower()
    
    # Exclude advertisements
    if is_advertisement(keyword):
        return None, False
    
    # Primary keywords are intent phrases (from templates)
    # They indicate someone wants to do something (buy, move, etc.)
    intent_indicators = [
        'buy', 'buying', 'moving', 'move', 'relocating', 'relocate',
        'should i', 'is it worth', 'thinking of', 'considering',
        'planning to', 'looking for', 'need help', 'want to',
        'trying to', 'can i afford', 'how much', 'what is it like',
        'any advice', 'any recommendations', 'help me decide',
        'first time', 'preapproval', 'pre-approval', 'making an offer',
        'rent vs buy', 'should i rent or buy', 'living in', 'renting in',
        'best neighborhoods', 'safe neighborhoods', 'areas to avoid',
        'schools', 'commute', 'traffic', 'wildfire', 'earthquake',
        'mortgage', 'down payment', 'closing costs', 'home inspection'
    ]
    
    # Check if template contains intent indicators
    template_lower = template.lower()
    has_intent = any(indicator in template_lower for indicator in intent_indicators)
    
    # Check if keyword itself contains intent indicators
    keyword_has_intent = any(indicator in keyword_lower for indicator in intent_indicators)
    
    # If keyword contains both intent AND place, it's a PRIMARY keyword
    # (e.g., "buy a house in Los Angeles" - this is the full search query)
    if (has_intent or keyword_has_intent) and place and place.lower() in keyword_lower:
        return 'primary', True
    
    # If keyword has intent but no place, it's still primary
    if has_intent or keyword_has_intent:
        return 'primary', True
    
    # If keyword is just a place name (no intent), it's secondary
    # This allows matching: primary="buy a house in" + secondary="Los Angeles"
    if place and place.lower() in keyword_lower and not (has_intent or keyword_has_intent):
        return 'secondary', True
    
    # Default: if it has a place, make it secondary; otherwise primary if template has intent
    if place and place.lower() in keyword_lower:
        return 'secondary', True
    
    # Fallback: if template has intent, it's primary
    if has_intent:
        return 'primary', True
    
    # Default to secondary for place-only keywords
    return 'secondary', True


def load_keywords(keywords_file: str, format_type: str = 'csv', batch_size: int = 1000):
    """Load keywords from file and insert into database"""
    db = SessionLocal()
    try:
        # Initialize database
        init_db()
        logger.info("Database initialized")
        
        repository = KeywordRepository(db)
        
        # Load existing keywords into memory to avoid duplicate queries
        logger.info("Loading existing keywords from database...")
        existing_keywords = set()
        existing = db.query(Keyword).all()
        for kw in existing:
            existing_keywords.add((kw.word.lower(), kw.type))
        logger.info(f"Found {len(existing_keywords)} existing keywords in database")
        
        # Load keywords from file
        logger.info(f"Loading keywords from {keywords_file} (format: {format_type})")
        if format_type == 'jsonl':
            keywords_data = load_keywords_from_jsonl(keywords_file)
        else:
            keywords_data = load_keywords_from_csv(keywords_file)
        
        logger.info(f"Loaded {len(keywords_data)} keywords from file")
        
        # Process keywords in batches
        primary_count = 0
        secondary_count = 0
        place_secondary_count = 0  # Track place names added as secondary
        skipped_ads = 0
        skipped_duplicates = 0
        skipped_invalid = 0
        
        keywords_to_insert = []
        total_processed = 0
        
        for kw_data in keywords_data:
            total_processed += 1
            
            # Progress logging every 10k keywords
            if total_processed % 10000 == 0:
                logger.info(f"Processed {total_processed:,}/{len(keywords_data):,} keywords... "
                          f"(Created: {primary_count + secondary_count}, Skipped: {skipped_ads + skipped_duplicates + skipped_invalid})")
            
            keyword = kw_data.get('keyword', '').strip()
            if not keyword:
                skipped_invalid += 1
                continue
            
            # Skip invalid keywords (headers, separators, etc.)
            if keyword.startswith('=') or keyword.startswith('-') or len(keyword) < 3:
                skipped_invalid += 1
                continue
            
            template = kw_data.get('template', '')
            place = kw_data.get('place1', '') or kw_data.get('place2', '')
            
            # Classify keyword
            keyword_type, is_valid = classify_keyword(keyword, template, place)
            
            if not is_valid:
                skipped_ads += 1
                continue
            
            if keyword_type is None:
                skipped_invalid += 1
                continue
            
            # Check if keyword already exists (in memory)
            keyword_key = (keyword.lower(), keyword_type)
            if keyword_key in existing_keywords:
                skipped_duplicates += 1
                continue
            
            # Add to batch
            keywords_to_insert.append({
                'word': keyword.lower().strip(),
                'type': keyword_type,
                'enabled': True
            })
            
            # Mark as existing to avoid duplicates in same batch
            existing_keywords.add(keyword_key)
            
            # ALSO create a secondary keyword for the place name if it exists
            # This ensures posts can match both primary (full phrase) and secondary (place)
            if place and place.strip() and keyword_type == 'primary':
                place_lower = place.lower().strip()
                # Skip invalid place names
                if (len(place_lower) > 2 and 
                    not place_lower.startswith('=') and 
                    not place_lower.startswith('-') and
                    place_lower not in ['use this', 'counties', 'cities', 'includes', 'notes', 'place']):
                    place_key = (place_lower, 'secondary')
                    if place_key not in existing_keywords:
                        keywords_to_insert.append({
                            'word': place_lower,
                            'type': 'secondary',
                            'enabled': True
                        })
                        existing_keywords.add(place_key)
                        place_secondary_count += 1  # Track separately for logging
            
            # Insert batch when it reaches batch_size
            if len(keywords_to_insert) >= batch_size:
                try:
                    # Bulk insert
                    db.bulk_insert_mappings(Keyword, keywords_to_insert)
                    db.commit()
                    
                    # Count by type
                    for kw in keywords_to_insert:
                        if kw['type'] == 'primary':
                            primary_count += 1
                        else:
                            secondary_count += 1
                    
                    keywords_to_insert = []
                except Exception as e:
                    db.rollback()
                    logger.warning(f"Batch insert failed, trying individual inserts: {str(e)}")
                    # Fallback to individual inserts for this batch
                    for kw in keywords_to_insert:
                        try:
                            result = repository.create_keyword(
                                word=kw['word'],
                                keyword_type=kw['type'],
                                enabled=kw['enabled']
                            )
                            if result:
                                if kw['type'] == 'primary':
                                    primary_count += 1
                                else:
                                    secondary_count += 1
                        except Exception as e2:
                            logger.warning(f"Failed to insert keyword {kw['word']}: {str(e2)}")
                    keywords_to_insert = []
        
        # Insert remaining keywords
        if keywords_to_insert:
            try:
                db.bulk_insert_mappings(Keyword, keywords_to_insert)
                db.commit()
                
                for kw in keywords_to_insert:
                    if kw['type'] == 'primary':
                        primary_count += 1
                    else:
                        secondary_count += 1
            except Exception as e:
                db.rollback()
                logger.warning(f"Final batch insert failed: {str(e)}")
        
        # Get final counts
        counts = repository.get_keyword_count()
        
        logger.info("=" * 60)
        logger.info("Keyword loading complete")
        logger.info(f"Processed {total_processed:,} keywords from file")
        logger.info(f"Created {primary_count:,} new primary keywords")
        logger.info(f"Created {secondary_count:,} new secondary keywords")
        logger.info(f"Created {place_secondary_count:,} place names as secondary keywords")
        logger.info(f"Skipped {skipped_ads:,} advertisement keywords")
        logger.info(f"Skipped {skipped_duplicates:,} duplicate keywords")
        logger.info(f"Skipped {skipped_invalid:,} invalid keywords")
        logger.info(f"Total keywords in database: {counts['total']:,} ({counts['primary']:,} primary, {counts['secondary']:,} secondary)")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"Failed to load keywords: {str(e)}", exc_info=True)
        raise
    finally:
        db.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Load generated keywords into database")
    parser.add_argument('--keywords', required=True, help="Path to keywords CSV or JSONL file")
    parser.add_argument('--format', default='csv', choices=['csv', 'jsonl'], help="File format (default: csv)")
    parser.add_argument('--batch-size', type=int, default=1000, help="Batch size for bulk inserts (default: 1000)")
    args = parser.parse_args()
    
    load_keywords(args.keywords, args.format, batch_size=args.batch_size)

